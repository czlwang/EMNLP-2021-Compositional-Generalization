% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@misc{kuo2020compositional,
	title={Compositional Networks Enable Systematic Generalization for Grounded Language Understanding}, 
	author={Yen-Ling Kuo and Boris Katz and Andrei Barbu},
	year={2020},
	eprint={2008.02742},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{heinze-think-2020,
	author    = {Christina Heinze{-}Deml and
	Diane Bouchacourt},
	title     = {Think before you act: {A} simple baseline for compositional generalization},
	journal   = {CoRR},
	volume    = {abs/2009.13962},
	year      = {2020},
	url       = {https://arxiv.org/abs/2009.13962},
	archivePrefix = {arXiv},
	eprint    = {2009.13962},
	timestamp = {Wed, 30 Sep 2020 16:16:22 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2009-13962.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gao-etal-2020-systematic,
	abstract = {Systematic Generalization refers to a learning algorithm{'}s ability to extrapolate learned behavior to unseen situations that are distinct but semantically similar to its training data. As shown in recent work, state-of-the-art deep learning models fail dramatically even on tasks for which they are designed when the test set is systematically different from the training data. We hypothesize that explicitly modeling the relations between objects in their contexts while learning their representations will help achieve systematic generalization. Therefore, we propose a novel method that learns objects{'} contextualized embeddings with dynamic message passing conditioned on the input natural language and end-to-end trainable with other downstream deep learning modules. To our knowledge, this model is the first one that significantly outperforms the provided baseline and reaches state-of-the-art performance on grounded SCAN (gSCAN), a grounded natural language navigation dataset designed to require systematic generalization in its test splits.},
	address = {Suzhou, China},
	author = {Gao, Tong and Huang, Qi and Mooney, Raymond},
	booktitle = {Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
	month = {dec},
	pages = {491--503},
	publisher = {Association for Computational Linguistics},
	title = {{Systematic Generalization on g{SCAN} with Language Conditioned Embedding}},
	url = {https://www.aclweb.org/anthology/2020.aacl-main.49},
	year = {2020}
}

@article{LiVisualBert2019,
	author    = {Liunian Harold Li and
	Mark Yatskar and
	Da Yin and
	Cho{-}Jui Hsieh and
	Kai{-}Wei Chang},
	title     = {VisualBERT: {A} Simple and Performant Baseline for Vision and Language},
	journal   = {CoRR},
	volume    = {abs/1908.03557},
	year      = {2019},
	url       = {http://arxiv.org/abs/1908.03557},
	archivePrefix = {arXiv},
	eprint    = {1908.03557},
	timestamp = {Mon, 19 Aug 2019 13:21:03 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1908-03557.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tan-bansal-2019-lxmert,
	abstract = {Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22{\%} absolute (54{\%} to 76{\%}). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert},
	address = {Hong Kong, China},
	author = {Tan, Hao and Bansal, Mohit},
	booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
	doi = {10.18653/v1/D19-1514},
	month = {nov},
	pages = {5100--5111},
	publisher = {Association for Computational Linguistics},
	title = {{{LXMERT}: Learning Cross-Modality Encoder Representations from Transformers}},
	url = {https://www.aclweb.org/anthology/D19-1514},
	year = {2019}
}


@inproceedings{LuViLBERT2019,
	author    = {Jiasen Lu and
	Dhruv Batra and
	Devi Parikh and
	Stefan Lee},
	editor    = {Hanna M. Wallach and
	Hugo Larochelle and
	Alina Beygelzimer and
	Florence d'Alch{\'{e}}{-}Buc and
	Emily B. Fox and
	Roman Garnett},
	title     = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations
	for Vision-and-Language Tasks},
	booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
	on Neural Information Processing Systems 2019, NeurIPS 2019, December
	8-14, 2019, Vancouver, BC, Canada},
	pages     = {13--23},
	year      = {2019},
	url       = {https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html},
	timestamp = {Thu, 21 Jan 2021 15:15:19 +0100},
	biburl    = {https://dblp.org/rec/conf/nips/LuBPL19.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Chen2020TopologicalPW,
	title={Topological Planning with Transformers for Vision-and-Language Navigation},
	author={Kevin Chen and Junshen Chen and Jo Chuang and Marynel V'azquez and S. Savarese},
	journal={ArXiv},
	year={2020},
	volume={abs/2012.05292}
}

@inproceedings{fang2019scene,
	title={Scene memory transformer for embodied agents in long-horizon tasks},
	author={Fang, Kuan and Toshev, Alexander and Fei-Fei, Li and Savarese, Silvio},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={538--547},
	year={2019}
}


@misc{magassouba2021crossmap,
	title={CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation}, 
	author={Aly Magassouba and Komei Sugiura and Hisashi Kawai},
	year={2021},
	eprint={2103.00852},
	archivePrefix={arXiv},
	primaryClass={cs.RO}
}

@article{ruis2020benchmark,
	title={A Benchmark for Systematic Generalization in Grounded Language Understanding},
	author={Ruis, Laura and Andreas, Jacob and Baroni, Marco and Bouchacourt, Diane and Lake, Brenden M},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	year={2020}
}

@inproceedings{andreas2017modular,
	title={Modular multitask reinforcement learning with policy sketches},
	author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
	booktitle={International Conference on Machine Learning},
	pages={166--175},
	year={2017},
	organization={PMLR}
}

@article{Kuo2020EncodingFA,
	title={Encoding formulas as deep networks: Reinforcement learning for zero-shot execution of LTL formulas},
	author={Yen-Ling Kuo and B. Katz and A. Barbu},
	journal={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	year={2020},
	pages={5604-5610}
}

@inproceedings{Lu2019ViLBERTPT,
	title={ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
	author={Jiasen Lu and Dhruv Batra and D. Parikh and Stefan Lee},
	booktitle={NeurIPS},
	year={2019}
}

@article{Hao2020TowardsLA,
	title={Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training},
	author={Weituo Hao and C. Li and Xiujun Li and L. Carin and Jianfeng Gao},
	journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	year={2020},
	pages={13134-13143}
}

@InProceedings{roma2019,
	author = {Patel, Roma and Pavlick, Roma and Tellex, Stefanie},
	booktitle = {NAACL 2019},
	series = {SpLU  RoboNLP Workshop},
	title = {Learning to Ground Language to Temporal Logical Form},
	year = {2019}
}